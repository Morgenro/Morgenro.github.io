---
title: financial-risk-LLM
date: 2025-07-15 01:19:45
tags: AI
description: é‡‘èé£é™©å¤§æ¨¡å‹è®­ç»ƒåŠç³»ç»Ÿå¼€å‘
---
æœ¬æ–‡æ˜¯æˆ‘çš„æ¯•è®¾é¡¹ç›®ï¼Œç”±äºæ˜¯ä»24å¹´6æœˆåˆ°25å¹´5æœˆé™†é™†ç»­ç»­è¿­ä»£å‡ºæ¥çš„ï¼Œé¡¹ç›®è¿‡äºå±å±±ï¼Œ~~åœ¨ç‹æŸçš„ç£ä¿ƒä¸‹~~é‡å¤´å¼€å§‹å¤åˆ»ä¸€éæµç¨‹

## é¡¹ç›®ç®€ä»‹

&emsp;&emsp; æ¯•è®¾é¡¹ç›®ä¸ºé‡‘èé£é™©å¤§æ¨¡å‹åŠç³»ç»Ÿå¼€å‘ã€‚è¯¦ç»†æŒ‡å¯¹æ‰€é€‰å…¬å¸çš„è´¢åŠ¡æŠ¥è¡¨åŠæ–°é—»èˆ†æƒ…ç»“åˆåˆ†æå¾—å‡ºç»¼åˆæ€§çš„åˆ†æ•°ï¼Œå†é€šè¿‡ç®€å•çš„å¯è§†åŒ–ç½‘ç«™è¾“å‡ºåˆ°å‰ç«¯\
&emsp;&emsp; å…³äºæ–°é—»èˆ†æƒ…éƒ¨åˆ†ï¼Œç®€å•æ¥è¯´å°±æ˜¯å¯¹æ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ã€‚æœ¬æ–‡é’ˆå¯¹Meta-Llama-3.1-8B-bnb-4bitæ¨¡å‹è¿›è¡Œä¿®æ”¹å¾®è°ƒã€‚åœ¨æ¨¡å‹æ–¹é¢åŠ å…¥ç‰¹å¾å€¼å’Œå…¨å±€è¯­ä¹‰çš„åŒé€šé“ç³»ç»Ÿå’Œç”Ÿæˆ-åˆ†ç±»æ¨¡å‹å¤´ã€‚åœ¨å‚æ•°è°ƒä¼˜æ–¹é¢åŠ å…¥LoRAå¹¶è¿›è¡Œåˆ†å±‚è§£å†»ã€‚åœ¨æ¨¡å‹è®­ç»ƒæ–¹é¢åŠ å…¥éƒ¨åˆ†å›è°ƒä»£ç ã€‚\
&emsp;&emsp; åœ¨ç»“åˆè´¢åŠ¡æŠ¥è¡¨åŠæ–°é—»èˆ†æƒ…éƒ¨åˆ†ï¼Œä½¿ç”¨Altman-z-scoreè®¡ç®—è´¢åŠ¡æŠ¥è¡¨çš„åˆ†æ•°ã€‚å¯¹æ¯æ—¥æ–°é—»è¿›è¡Œæ—¶åºåˆ†æ•°è¡°å‡åå¾—å‡ºçš„æ¯æ—¥æƒ…æ„Ÿåˆ†æ•°ç›¸åŠ å†å’Œè´¢åŠ¡æŠ¥è¡¨çš„z-scoreåˆ†æ•°æƒé‡ç›¸åŠ ã€‚å¾—å‡ºæœ€åçš„ç»“æœã€‚\

## ç¯å¢ƒé…ç½®

```python requirement.txt
torch==2.5.1+cu121
transformers==4.46.1
datasets==2.18.0
peft==0.12.0
bitsandbytes==0.45.2
accelerate==0.34.1
pandas==2.2.3
matplotlib==3.9.2
tqdm==4.67.1
scikit-learn==1.6.1
seaborn==0.13.2
sentencepiece==0.2.0
xformers==0.0.29.post3
evaluate==0.4.3
trl==0.8.6
safetensors==0.5.2
```
&emsp;&emsp; æœ¬æ–‡æ˜¯åœ¨[unsloth-llama-3-8b-bnb-4bit](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)çš„åˆå§‹æŒ‡å¯¼æ–‡ç« çš„åŸºç¡€ä¸‹è¿›è¡Œä¿®æ”¹çš„

## unslothåŸæ–‡è§£é‡Š

&emsp;&emsp; å…ˆè§£é‡Šä¸€ä¸‹unsloth-llama-3-8b-bnb-4bitçš„åŸæ–‡
```python  
from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 15 trillion tokens model 2x faster!
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # We also uploaded 4bit for 405b!
    "unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # New Mistral 12b 2x faster!
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!
] # More models at https://huggingface.co/unsloth
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
```
&emsp;&emsp; è¿™æ®µçš„ç›®çš„æ˜¯åŠ è½½ç»è¿‡unslothé‡åŒ–åçš„æ¨¡å‹ï¼ŒåŸæ–‡åœ¨colabè¿è¡Œï¼Œå¯ä»¥ç›´è¿åˆ°huggingfaceï¼Œç”±äºå¢™çš„åŸå› ï¼Œç›´æ¥è¿è¡Œæ—¶æ— æ³•ä¸<http://huggingface.io>å–å¾—è”ç³»ï¼Œå»ºè®®é€šè¿‡é•œåƒä¸‹è½½æ¨¡å‹åæœ¬åœ°å¯¼å…¥æ¨¡å‹ã€‚
```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
    alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
```
&emsp;&emsp; è¿™æ®µæ˜¯åŠ è½½äº†LoRAè¿›è¡Œå‚æ•°å¾®è°ƒã€‚ä¼—æ‰€å‘¨çŸ¥ï¼ŒLoRAå¯ä»¥å¤§å¹…å‡å°‘æ‰€éœ€è¦æ›´æ–°çš„å‚æ•°ã€‚è¿™åœ¨æˆ‘ä»¬å°è§„æ¨¡è®­ç»ƒå¾®è°ƒæ–¹é¢ååˆ†é‡è¦ã€‚\
&emsp;&emsp; é‡‡ç”¨r=16ï¼ˆæŒ‡LoRA rankï¼Œè¶Šå¤§è¶Šç²¾å‡†ï¼ŒåŒæ—¶è®­ç»ƒçš„å‚æ•°ä¹Ÿè¶Šé«˜ï¼Œåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­ï¼Œå½“ç§©è¾¾åˆ°8æˆ–16æ—¶ï¼Œæ¨¡å‹æ€§èƒ½ï¼ˆå¦‚å‡†ç¡®ç‡ã€å›°æƒ‘åº¦ï¼‰å·²æ¥è¿‘å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰çš„æ°´å¹³ã€‚ï¼‰

>å‚è€ƒæ–‡çŒ®ï¼šHu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. ICLR, 2022, 1(2): 3.

&emsp;&emsp; é’ˆå¯¹æŠ•å½±å±‚ä¸­çš„["q_proj", "k_proj", "v_proj", "o_proj"ï¼Œ"gate_proj", "up_proj", "down_proj"]æ¨¡å—è¿›è¡Œå¾®è°ƒï¼Œ\
&emsp;&emsp; lora_alpha = 16ï¼Œlora_alphaæ˜¯ä¸€ä¸ªç”¨äºç¼©æ”¾LoRAæ›´æ–°çš„ç³»æ•°ã€‚åœ¨è®¡ç®—æƒé‡æ›´æ–°$\Delta W$æ—¶ï¼Œå®ƒèµ·åˆ°è°ƒæ•´æ›´æ–°å¹…åº¦çš„ä½œç”¨

$\Delta W = \frac{\text{lora alpha}}{r} AB$

&emsp;&emsp; random_state æŒ‡éšæœºç§å­ï¼Œå›ºå®šç§å­å¯ä»¥ä¿è¯è¿è¡Œç»“æœå¯å¤ç°

```python

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass
```
&emsp;&emsp; è¿™æ˜¯æç¤ºè¯æ¨¡æ¿ï¼Œå¯ä»¥æ ¹æ®ä½ æ‰€éœ€è¦çš„é¢†åŸŸè¿ç§»ä¿®æ”¹æç¤ºè¯
```python
from datasets import load_dataset
dataset = load_dataset("yahma/alpaca-cleaned", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True,)
)
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)
trainer_stats = trainer.train()
```
&emsp;&emsp; è¿™å°±æ˜¯æ­£å¼çš„å¼€å§‹è®­ç»ƒçš„ä¸»æµç¨‹äº†ã€‚\
&emsp;&emsp; ä¸‹é¢è¯¦ç»†è§£æä¼ å…¥SFTTrainerçš„å‚æ•°ï¼š\
modelï¼šå³å…ˆå‰åŠ è½½çš„è¯­è¨€æ¨¡å‹å¯¹è±¡\
tokenizerï¼šæ˜¯ä¸æ¨¡å‹å…³è”çš„åˆ†è¯å™¨å¯¹è±¡ï¼Œç”¨äºå¤„ç†æ–‡æœ¬æ•°æ®ã€‚\
train_datasetï¼šè¿™æ˜¯ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ï¼Œ\
dataset_text_fieldï¼šæŒ‡å®šæ•°æ®é›†ä¸­åŒ…å«è®­ç»ƒæ–‡æœ¬æ•°æ®çš„åˆ—åï¼Œåœ¨æ­¤ä¾‹ä¸­ä¸º "text"ã€‚\
max_seq_lengthï¼šè®¾å®šå¤„ç†è¾“å…¥æ•°æ®çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œæœ‰åŠ©äºåœ¨è®­ç»ƒæ—¶æ§åˆ¶å†…å­˜ä½¿ç”¨ã€‚\
dataset_num_procï¼šç¡®å®šç”¨äºæ•°æ®åŠ è½½å’Œé¢„å¤„ç†çš„è¿›ç¨‹æ•°é‡ï¼Œå¢åŠ è¿›ç¨‹æ•°å¯åŠ å¿«æ•°æ®å‡†å¤‡é€Ÿåº¦ã€‚

argsï¼šä¼ å…¥TrainingArgumentså¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒè¿‡ç¨‹çš„è¯¦ç»†é…ç½®ã€‚\
&emsp;&emsp; ä¸‹é¢çœ‹çœ‹TrainingArgumentså¯¹è±¡ä¸­çš„å‚æ•°ï¼š\
per_device_train_batch_sizeï¼šè®¾å®šæ¯ä¸ªè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ä¸Šçš„è®­ç»ƒæ‰¹é‡å¤§å°ï¼Œè¾ƒå°çš„æ‰¹é‡æ‰€éœ€å†…å­˜æ›´å°‘ï¼ŒåŒæ—¶æ›´æ…¢ã€‚\
gradient_accumulation_stepsï¼šè¯¥å‚æ•°å…è®¸åœ¨æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–æ­¥éª¤å‰ï¼Œå¯¹å¤šä¸ªè¾ƒå°æ‰¹é‡çš„æ¢¯åº¦è¿›è¡Œç´¯ç§¯ï¼Œè¿™æ ·èƒ½åœ¨ä¸å¢åŠ å•ä¸ªæ‰¹é‡å†…å­˜éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆå¢å¤§æ•´ä½“æ‰¹é‡å¤§å°ã€‚\
warmup_stepsï¼šæŒ‡å®šè®­ç»ƒå¼€å§‹æ—¶çº¿æ€§é¢„çƒ­é˜¶æ®µçš„æ­¥æ•°ï¼Œåœ¨æ­¤é˜¶æ®µå­¦ä¹ ç‡ä¼šé€æ¸æé«˜ã€‚\
max_stepsï¼šè®¾å®šè¦æ‰§è¡Œçš„æ€»è®­ç»ƒæ­¥æ•°ï¼Œåœ¨\è¿™æ®µä»£ç ä¸­è®¾ä¸º 60ï¼Œæ˜¯å‡ºäºæ¼”ç¤ºç›®çš„ã€‚è‹¥è¦è¿›è¡Œå®Œæ•´è®­ç»ƒï¼Œé€šå¸¸åº”è®¾ç½®train_epochsï¼Œè€Œå°†max_stepsè®¾ä¸ºNoneã€‚\
learning_rateï¼šä¼˜åŒ–å™¨ä½¿ç”¨çš„å­¦ä¹ ç‡ã€‚\
logging_stepsï¼šæŒ‡å®šè®°å½•è®­ç»ƒè¿›åº¦ï¼ˆå¦‚æŸå¤±å€¼ã€å­¦ä¹ ç‡ï¼‰çš„é¢‘ç‡ï¼Œè¿™é‡Œæ¯ 1 æ­¥è®°å½•ä¸€æ¬¡ã€‚\
optimï¼šè®­ç»ƒä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼Œ"adamw_8bit" æ˜¯ä¸€ç§èŠ‚çœå†…å­˜çš„ AdamW ä¼˜åŒ–å™¨ã€‚\
weight_decayï¼šæ­£åˆ™åŒ–å‚æ•°ï¼Œå¯¹è¾ƒå¤§çš„æƒé‡è¿›è¡Œæƒ©ç½šã€‚\
lr_scheduler_typeï¼šä½¿ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œ"linear" è¡¨ç¤ºå­¦ä¹ ç‡åœ¨é¢„çƒ­é˜¶æ®µåå°†çº¿æ€§ä¸‹é™ã€‚\
seedï¼šè®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°ã€‚\
output_dirï¼šä¿å­˜è®­ç»ƒè¾“å‡ºï¼ˆå¦‚æ£€æŸ¥ç‚¹å’Œæ—¥å¿—ï¼‰çš„ç›®å½•ã€‚

## æ•°æ®é›†æ„å»º
&emsp;&emsp; ç”±äºç½‘ä¸Šå¹¶æ²¡æœ‰ä¸­æ–‡ç°æˆçš„æ ‡æ³¨å¥½çš„æ•°æ®é›†ï¼Œå› æ­¤è¦è‡ªå·±æ„å»º\
&emsp;&emsp; æœ€åˆæ˜¯æƒ³è¦ä»[é‡‘èchoiceæ•°æ®](https://choice.eastmoney.com/)copyæ•°æ®é›†,however,24å¹´æ—¶å€™è¿˜å¯ä»¥500æ¡ä¸€æ¬¡å¤åˆ¶ä¸€é¡µçš„æ•°æ®(ç›´æ¥å¯¼å…¥æ•°æ®æœ‰æ¬¡æ•°é™åˆ¶,å¯ä»¥ç›´æ¥åœ¨é¡µé¢å¤åˆ¶æ•°æ®åˆ°excelè¡¨æ ¼),åˆ°äº†25å¹´æ—¶å€™åªèƒ½50æ¡ä¸€é¡µçš„å¤åˆ¶æ•°æ®äº†,éå¸¸çš„éº»çƒ¦,å°±å†™äº†ä¸ªç®€å•çš„çˆ¬è™«çˆ¬å–[ä¸œæ–¹è´¢å¯Œç½‘è‚¡å§](https://guba.eastmoney.com/)èµ„è®¯é¡µé¢çš„æ•°æ®
&emsp;&emsp;ä¸‹é¢æ˜¯éƒ¨åˆ†ä»£ç 
### éç»“æ„åŒ–æ•°æ®é›†
```python 
# è‚¡ç¥¨ä»£ç å’Œåç§°æ˜ å°„
STOCK_CODES = [
    {'code': 'HK00700', 'name': 'è…¾è®¯'},
    {'code': 'HK09888', 'name': 'ç™¾åº¦'},
    {'code': 'HK09999', 'name': 'ç½‘æ˜“'},
    {'code': 'HK01688', 'name': 'é˜¿é‡Œå·´å·´'}
]
# çˆ¬è™«è®¾ç½®
CRAWLER_SETTINGS = {
    'start_page': 1,
    'end_page': 2,
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Edg/115.0.1901.183'
    }
}
class NewsCrawler:
    def process_date(self, date_str):
        """å¤„ç†æ—¶é—´æ ¼å¼å¹¶æ¨æ–­å¹´ä»½"""
        # æå–æœˆæ—¥
        match = re.search(r'(\d{2})-(\d{2})', date_str)
        if not match:
            return "error"
        
        month, day = int(match.group(1)), int(match.group(2))
        
        # è·å–å½“å‰æ—¥æœŸ
        current_date = datetime.datetime.now()
        current_month = current_date.month
        
        # é»˜è®¤å¹´ä»½ä¸ºå½“å‰å¹´ä»½
        year = current_date.year
        
        # å¦‚æœå½“å‰æœˆä»½å°äºçˆ¬å–çš„æœˆä»½ï¼Œè¯´æ˜æ˜¯å»å¹´çš„æ•°æ®
        if month > current_month:
            year -= 1
        
        # æ ¼å¼åŒ–æ—¥æœŸä¸ºYYYY-MM-DD
        return f"{year}-{month:02d}-{day:02d}"
    
    def crawl_news(self):
        """çˆ¬å–æ‰€æœ‰è‚¡ç¥¨çš„æ–°é—»æ•°æ®"""
        all_data = {}
        
        for stock in self.stock_codes:
            stock_code = stock['code']
            stock_name = stock['name']
            
            print(f"å¼€å§‹çˆ¬å– {stock_name} çš„æ–°é—»æ•°æ®...")
            
            # ç”¨äºå­˜å‚¨è¯¥è‚¡ç¥¨çš„æ‰€æœ‰æ–°é—»æ•°æ®
            stock_data = []
            # åˆ›å»ºé›†åˆç”¨äºå­˜å‚¨å·²çˆ¬å–çš„URLï¼Œé¿å…é‡å¤
            processed_urls = set()
            
            for page in range(self.start_page, self.end_page + 1):
                # æ„å»ºURL
                if page == 1:
                    url = f'https://guba.eastmoney.com/list,{stock_code.lower()},1,f.html'
                else:
                    url = f'https://guba.eastmoney.com/list,{stock_code.lower()},1,f_{page}.html'
                
                print(f"æ­£åœ¨çˆ¬å– {stock_name} ç¬¬ {page} é¡µ: {url}")
                
                try:
                    request = urllib.request.Request(url=url, headers=self.headers)
                    response = urllib.request.urlopen(request)
                    content = response.read()
                    tree = etree.HTML(content)
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ•°æ®
                    items = tree.xpath('//tbody[@class="listbody"]/tr')
                    print(f"åœ¨ {stock_name} ç¬¬ {page} é¡µæ‰¾åˆ° {len(items)} æ¡æ•°æ®")
                    
                    if len(items) == 0:
                        print(f"è­¦å‘Š: {stock_name} ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ç½‘é¡µç»“æ„æˆ–URLæ ¼å¼")
                        continue
                    
                    # è§£ææ•°æ®
                    for item in items:
                        # æå–æ ‡é¢˜
                        title = item.xpath('.//div[@class="title"]//a/text()')
                        title = title[0].strip() if title else 'error'
                        
                        # æå–æ›´æ–°æ—¶é—´å¹¶å¤„ç†
                        update = item.xpath('.//div[@class="update"]/text()')
                        update = update[0].strip() if update else 'error'
                        
                        # å¤„ç†æ—¥æœŸæ ¼å¼
                        formatted_date = self.process_date(update)
                        
                        # æå–é“¾æ¥ç”¨äºå»é‡
                        link = item.xpath('.//div[@class="title"]//a/@href')
                        if link:
                            link = link[0].strip()
                            if link.startswith('/news'):
                                link = "https://guba.eastmoney.com" + link
                            elif link.startswith('//caifuhao.eastmoney.com'):
                                link = "https:" + link
                            else:
                                link = 'error'
                        else:
                            link = 'error'
```
&emsp;&emsp;ç”±äºè‚¡å§æœ¬èº«é¡µé¢çš„urlå¹¶æ²¡æœ‰æ˜¾ç¤ºå¹´ä»½,æ‰€ä»¥è‡ªå·±è®¾ç½®äº†ä¸€ä¸ªæ¨æ–­å¹´ä»½çš„éƒ¨åˆ†:é»˜è®¤ç¬¬ä¸€é¡µæ•°æ®æ˜¯ä»Šå¹´çš„,è¶Šå¾€åæœˆä»½è¶Šå¤§,ç›´åˆ°æœˆä»½è½¬ä¸º1æœˆ,è¿™æ—¶å€™è®¤ä¸ºæ˜¯å»å¹´çš„,å¹´ä»½å‡ä¸€\
&emsp;&emsp;ä»å½“å‰å…ƒç´ ï¼ˆitemï¼‰ä¸‹æŸ¥æ‰¾æ‰€æœ‰ \<div class="title"> çš„å­å…ƒç´ ï¼Œå†å‘ä¸‹é€’å½’æŸ¥æ‰¾æ‰€æœ‰ \<a> æ ‡ç­¾çš„ href å±æ€§ã€‚è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼ˆå³ä½¿åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼‰ã€‚link[0] è¡¨ç¤ºåªå–ç¬¬ä¸€ä¸ªåŒ¹é…çš„é“¾æ¥ï¼ˆå‡è®¾ç›®æ ‡é¡µé¢ä¸­æ¯ä¸ªæ ‡é¢˜åªæœ‰ä¸€ä¸ªæœ‰æ•ˆé“¾æ¥ï¼‰ã€‚strip() ç”¨äºæ¸…ç†é“¾æ¥ä¸¤ç«¯çš„ç©ºæ ¼æˆ–æ¢è¡Œç¬¦ã€‚ç„¶åè¡¥å…¨é“¾æ¥\
ä½ è¦åˆ†æå“ªä¸ªæ•°æ®çš„å…¬å¸çš„æ•°æ®å°±çˆ¬å…¬å¸çš„æ•°æ®,å†çˆ¬ç‚¹å…¶ä»–å…¬å¸çš„æ•°æ®ç»™:deepseek,è®©deepseekç»™ä½ "äººå·¥"æ ‡æ³¨æ•°æ®æœ€å7:3å½¢æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
#### è®­ç»ƒé›†å’ŒéªŒè¯é›†
&emsp;&emsp;æ”¶é›†å¥½çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†like:
```csv
time,text,label
2025-03-28,ç™¾åº¦Apolloå¯åŠ¨â€œæ˜Ÿç«è®¡åˆ’â€ æ­¦æ±‰å¤§å­¦è·èµ é¦–æ‰¹8è¾†è‡ªåŠ¨é©¾é©¶è½¦è¾†,1
2025-03-28,ç¾è‚¡ç›˜å‰çƒ­é—¨ä¸­æ¦‚è‚¡å¤šæ•°èµ°ä½ï¼Œè”šæ¥ã€ç™¾åº¦è·Œè¶…2%,0
2025-03-28,å¹¿å®‰é—¨åŒ»é™¢ã€ç™¾åº¦æ™ºèƒ½äº‘ä¸å…¨è¯ŠåŒ»å­¦è”åˆå‘å¸ƒä¸­åŒ»åŒ»ç–—æœåŠ¡å¤§æ¨¡å‹â€œå¹¿åŒ»Â·å²æ™ºâ€,1
2025-03-28,å¹¿å®‰é—¨åŒ»é™¢ã€ç™¾åº¦æ™ºèƒ½äº‘ç­‰è”åˆå‘å¸ƒä¸­åŒ»åŒ»ç–—æœåŠ¡å¤§æ¨¡å‹â€œå¹¿åŒ»Â·å²æ™ºâ€,1
2025-03-28,äº’è”ç½‘å¤§å‚æ¿€æˆ˜å¾®çŸ­å‰§ï¼çˆ±å¥‡è‰ºæ”¹ç¼–ç™¾éƒ¨æ¸¯ç‰‡IP ç™¾åº¦å¹´å†…å°†ç ¸é’±è¿‡äº¿,2
2025-03-28,DeepSeekæ¢æ–‡é”‹é¦–æ¬¡ç™»ä¸Šå…¨çƒå¯Œè±ªæ¦œï¼›ç™¾åº¦æ˜†ä»‘èŠ¯ä¸‰ä¸‡å¡é›†ç¾¤å³å°†ä¸Šçº¿ï½œæ•°æ™ºæ—©å‚,1
```

#### å¾…å¤„ç†æ–°é—»
&emsp;&emsp;æ”¶é›†å¥½çš„å¾…å¤„ç†æ–°é—»like:
```
time,text
2025-4-30,ä¸½äººä¸½å¦†â€œå¤±å® â€é­é˜¿é‡Œç³»æ¸…ä»“ å¥—ç°è¿‘5äº¿å…ƒ ç¥ç§˜æ¥ç›˜æ–¹ä¸Šä¸ªæœˆæ‰æˆç«‹
2025-4-30,å—å‘èµ„é‡‘è¿½è¸ªï½œ4æœˆå¤§ä¸¾åŠ ä»“é˜¿é‡Œå’Œè…¾è®¯ å¹´å†…æµå…¥è¶…6000äº¿æ¸¯å…ƒåŒæ¯”å¢è¿‘3å€
2025-4-30,å¥—ç°4.86äº¿ï¼Œåè®®è½¬è®©ä¸½äººä¸½å¦†17.57%è‚¡ä»½ï¼šé˜¿é‡Œç³»èµ„æœ¬å®Œæˆé€€å‡º
2025-4-30,ç™»é¡¶å…¨çƒæœ€å¼ºå¼€æºæ¨¡å‹ï¼šé˜¿é‡Œå®£å¸ƒå¼€æºQwen3
2025-4-29,è¶…è¶ŠDeepSeek-R1ï¼åƒé—®3ç™»é¡¶å…¨çƒæœ€å¼ºå¼€æºæ¨¡å‹ï¼Œé˜¿é‡Œ3800äº¿AIå¸ƒå±€å›¾è°±æµ®ç°
2025-4-29,æ—©æŠ¥ï½œç‰¹æœ—æ™®æ‹Ÿæ”¾æ¾å¤–å›½æ±½è½¦å…³ç¨ï¼›æœ€å¼ºå¼€æºæ¨¡å‹ï¼é˜¿é‡Œå‘å¸ƒå¹¶å¼€æºQwen3
2025-4-29,æ¸¯è‚¡æ—©æŠ¥ï½œé˜¿é‡Œå·´å·´å‘å¸ƒå¹¶å¼€æºæ–°ç‰ˆå¤§æ¨¡å‹Qwen3 èµ›åŠ›æ–¯é€’äº¤æ¸¯è‚¡ä¸Šå¸‚ç”³è¯·
2025-4-28,å—å‘èµ„é‡‘è¿½è¸ªï½œå‡€ä¹°å…¥è¶…20äº¿æ¸¯å…ƒ é‡æ–°åŠ ä»“ä¸¤åªETFå¤§å¹…æµå‡ºé˜¿é‡Œå·´å·´
```

### ç»“æ„åŒ–æ•°æ®é›†
&emsp;&emsp;éšä¾¿æ‰¾ä¸ªç½‘ç«™å°±èƒ½æ‰¾åˆ°ä¸Šå¸‚ä¼ä¸šçš„å­£åº¦çš„è´¢åŠ¡æŠ¥è¡¨,è¿™é‡Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨åˆ©æ¶¦è¡¨å’Œèµ„äº§è´Ÿå€ºè¡¨ä¸­çš„éƒ¨åˆ†æ•°æ®æ¥è®¡ç®—Altman-z-score
| Altman Z-Score   | æ‰€éœ€æ•°æ®é¡¹  |æ¥æºæŠ¥è¡¨|å…·ä½“å­—æ®µåç§°|
|  ----  | ----  |----|----|
| Xâ‚  | æµåŠ¨èµ„äº§ |èµ„äº§è´Ÿå€ºè¡¨|æµåŠ¨èµ„äº§åˆè®¡|
| Xâ‚  | æµåŠ¨è´Ÿå€º |èµ„äº§è´Ÿå€ºè¡¨|æµåŠ¨èµ„äº§åˆè®¡|
| Xâ‚  | æ€»èµ„äº§ |èµ„äº§è´Ÿå€ºè¡¨|æµåŠ¨è´Ÿå€ºåˆè®¡|
| Xâ‚‚  | ç•™å­˜æ”¶ç›Š |èµ„äº§è´Ÿå€ºè¡¨|å‚¨å¤‡|
| Xâ‚‚	|æ€»èµ„äº§	|èµ„äº§è´Ÿå€ºè¡¨	|æ€»èµ„äº§|
|Xâ‚ƒ|	æ¯ç¨å‰åˆ©æ¶¦|	åˆ©æ¶¦è¡¨	|è¥ä¸šåˆ©æ¶¦+åˆ©æ¯æ”¯å‡ºâ€“åˆ©æ¯æ”¶å…¥|
|Xâ‚ƒ	|æ€»èµ„äº§|	èµ„äº§è´Ÿå€ºè¡¨	|æ€»èµ„äº§|
|Xâ‚„	|è‚¡ä¸œæƒç›Š|	èµ„äº§è´Ÿå€ºè¡¨	|è‚¡ä¸œæƒç›Šåˆè®¡|
|Xâ‚„	|æ€»è´Ÿå€º	|èµ„äº§è´Ÿå€ºè¡¨	|æ€»è´Ÿå€ºåˆè®¡|
|Xâ‚…	|è¥ä¸šæ€»æ”¶å…¥	|åˆ©æ¶¦è¡¨	|è¥ä¸šæ€»æ”¶å…¥|
|Xâ‚…	|æ€»èµ„äº§	|èµ„äº§è´Ÿå€ºè¡¨	|æ€»èµ„äº§|



## é‡‘èæƒ…ç»ªåˆ†ææ¨¡å‹
&emsp;&emsp;æœ€åˆå¹¶æ²¡æœ‰å¯¹æ¨¡å‹æœ¬èº«è¿›è¡Œä¿®æ”¹,~~è¢«å¯¼å¸ˆç‹ ç‹ çš„æ‹·æ‰“~~,äºæ˜¯å‚è€ƒFinBERT: A Large Language Model for Extracting Information from Financial Text æ·»åŠ äº†ä¸€ä¸ªç”Ÿæˆè½¬åˆ†ç±»æ¨¡å‹å¤´ã€‚å‚è€ƒè¿‡å¾€çš„é‡‘èæ–°é—»æ–‡æœ¬,æ·»åŠ äº†ä¸€å †å…³é”®è¯(é‡‘èæ–°é—»æ„Ÿè§‰æ²¡ä»€ä¹ˆæ–°ä¸œè¥¿),å¯¹é‡‘èæ–‡æœ¬æå–[ç§¯æ,ä¸­æ€§,æ¶ˆæ]å‘é‡,å’Œæœ¬èº«å¤§æ¨¡å‹æ³¨æ„åŠ›æœºåˆ¶èåˆ,è¾“å‡ºæƒ…æ„Ÿåˆ†ç±»ã€‚
ä¸‹é¢æ˜¯éƒ¨åˆ†å…³é”®ä»£ç 

### æ¨¡å‹å¢å¼ºæ¶æ„
```python
class EnhancedLlamaClassifier(torch.nn.Module):
    def __init__(self, base_model):
        super().__init__()
        # åˆå§‹åŒ–LoRAé€‚é…å™¨
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        self.base_model = get_peft_model(base_model, lora_config)
        
        # è§£å†»LoRAå‚æ•°å¹¶ç¡®ä¿ä¸ºæµ®ç‚¹ç±»å‹
        for param in self.base_model.parameters():
            if param.requires_grad:
                param.requires_grad = False
        for name, param in self.base_model.named_parameters():
            if "lora" in name:
                param.requires_grad = True
                if not torch.is_floating_point(param):
                    param.data = param.data.float()

        # ç‰¹å¾å¢å¼ºæ¨¡å—
        self.feature_enhancer = torch.nn.Sequential(
            torch.nn.Linear(3, 16),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.1)
        )
        
        # åˆ†ç±»å™¨æ¨¡å—
        self.classifier = torch.nn.Sequential(
            torch.nn.Linear(base_model.config.hidden_size + 16, 512),
            torch.nn.GELU(),
            torch.nn.Dropout(0.2),
            torch.nn.Linear(512, 3)
        )

    def forward(self, input_ids, attention_mask, keyword_features=None, labels=None):
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        last_hidden = outputs.hidden_states[-1]
        pooled = last_hidden[:, 0, :]
        
        if keyword_features is not None:
            enhanced = self.feature_enhancer(keyword_features.float())
            pooled = torch.cat([pooled, enhanced], dim=1)

        logits = self.classifier(pooled)

        loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss(weight=torch.tensor([1.2, 1.0, 1.5]).cuda())
            loss = loss_fct(logits.view(-1, 3), labels.view(-1))
        
        return {"loss": loss, "logits": logits}
```
æ¨¡å‹ç»§æ‰¿torch.nn.Moduleï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

LoRAé€‚é…å™¨ï¼šè½»é‡çº§å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹,å…¸å‹LoRAå®ç°æ¨¡å¼:åªè®­ç»ƒLoRAå‚æ•°ï¼Œä¿æŒåŸå§‹æ¨¡å‹å‚æ•°å†»ç»“ã€‚\
ç‰¹å¾å¢å¼ºæ¨¡å—ï¼šå¤„ç†å¤–éƒ¨ç‰¹å¾,å°†ç¦»æ•£å…³é”®è¯ç‰¹å¾æ˜ å°„åˆ°ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½çš„è¿ç»­ç©ºé—´ã€‚æ¨¡å‹ä¼šæ›´ç²¾ç¡®çš„åˆ†æé‡‘èæ–‡æœ¬æƒ…æ„Ÿç‰¹å¾(ååˆ†å¥½ç”¨)\
åˆ†ç±»å™¨ï¼šç»¼åˆä¸¤ç§ç‰¹å¾è¿›è¡Œåˆ†ç±»\
æœ€åå¯¹ä¸å¹³è¡¡çš„æ ·æœ¬(çœ‹æ¶¨6000Â±,çœ‹è·Œ2000Â±,ä¸­æ€§1000Â±)è¿›è¡Œäº¤å‰ç†µåŠ æƒ,ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

### æ¨¡å‹å›è°ƒå‡½æ•°
```python
# ================= æ¢¯åº¦ç›‘æ§ =================
class GradientMonitor(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 50 == 0:
            grads = [
                p.grad.norm().item()
                for p in kwargs["model"].parameters()
                if p.grad is not None
            ]
            if grads:
                avg_grad = sum(grads) / len(grads)
                print(f"\næ¢¯åº¦ç›‘æ§ï¼ˆæ­¥éª¤ {state.global_step}ï¼‰: å¹³å‡èŒƒæ•°={avg_grad:.4f}")

# ================= æ—©åœæœºåˆ¶ =================
class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, 
                 target_loss=0.001, 
                 early_stopping_patience=10,
                 min_steps=100):
        self.target_loss = target_loss
        self.early_stopping_patience = early_stopping_patience
        self.min_steps = min_steps
        self.last_loss = float('inf')
        self.patience_count = 0

    def on_log(self, args, state, control, logs=None, **kwargs):
        if "eval_loss" in logs:
            return
        
        current_loss = logs.get("loss", None)
        if current_loss is None:
            return

        if state.global_step >= self.min_steps:
            if current_loss < self.target_loss:
                print(f"\nğŸš€ è®­ç»ƒæŸå¤±å·²è¾¾ç›®æ ‡å€¼ {current_loss:.4f} < {self.target_loss}")
                control.should_training_stop = True
                return

            if current_loss >= self.last_loss:
                self.patience_count += 1
                if self.patience_count >= self.early_stopping_patience:
                    print(f"\nâ¹ï¸ è¿ç»­ {self.early_stopping_patience} æ¬¡æŸå¤±æœªä¸‹é™")
                    control.should_training_stop = True
            else:
                self.last_loss = current_loss
                self.patience_count = 0
```
&emsp;&emsp;åœ¨æ¨¡å‹è®­ç»ƒåˆæœŸ,~~è®­ç»ƒæ•ˆæœä¸€å¨å±~~,æ¢¯åº¦ç›‘æ§æ–¹ä¾¿è°ƒå‚ã€‚é€‰æ‹©L2èŒƒæ•°è€Œéæœ€å¤§å€¼ï¼Œé¿å…æç«¯å€¼å¹²æ‰°\
&emsp;&emsp;æ—©åœæœºåˆ¶åœ¨ä¿è¯æ¨¡å‹å‡†ç¡®ç‡è¶‹äºæœ€é«˜æ—¶å°½æ—©ç»“æŸ~~å› ä¸ºAutoDLçš„æœºå­é’ˆå¯¹å¾ˆè´µ~~æ—©åœç­–ç•¥åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œå¹³å‡å‡å°‘50åˆ†é’Ÿçš„æ— æ•ˆè®­ç»ƒã€‚


### è®­ç»ƒä¸»æµç¨‹
```python
# ================= æ•°æ®åŠ è½½ =================
        print("ğŸ“¦ åŠ è½½æ•°æ®...")
        raw_dataset = datasets.load_dataset(
            "csv",
            data_files={
                "train": "/root/autodl-tmp/data/train_fixed.csv",
                "validation": "/root/autodl-tmp/data/valid_fixed.csv",
            },
        )

        # æ•°æ®é¢„å¤„ç†æµç¨‹
        def preprocess_function(examples):
            processed = EnhancedDataProcessor.process_batch(examples)
            tokenized = tokenizer(
                processed["text"],
                max_length=512,
                truncation=True,
                padding="max_length",
                add_special_tokens=True,
            )
            if len(processed["text"]) > 0:
                print("\nğŸ” æ ·æœ¬æ£€æŸ¥ï¼ˆå‰3ä¾‹ï¼‰:")
                for i in range(3):
                    print(f"Text {i+1}: {processed['text'][i][:150]}...")

            return {
                "input_ids": tokenized["input_ids"],
                "attention_mask": tokenized["attention_mask"],
                "labels": processed["label"],
                "keyword_features": processed["keyword_features"],
            }

        processed_dataset = raw_dataset.map(
            preprocess_function,
            batched=True,
            batch_size=500,
            remove_columns=raw_dataset["train"].column_names,
            load_from_cache_file=False,
        )

        # ================= è®­ç»ƒé…ç½® =================
        training_args = TrainingArguments(
            per_device_train_batch_size=2,  # é™ä½batch size
            gradient_accumulation_steps=16,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
            num_train_epochs=5,  # å‡å°‘è®­ç»ƒè½®æ¬¡
            learning_rate=1.5e-4,  # è°ƒæ•´å­¦ä¹ ç‡
            warmup_ratio=0.2,
            weight_decay=0.01,
            lr_scheduler_type="cosine",
            optim="adamw_torch_fused",
            evaluation_strategy="steps",
            eval_steps=100,
            logging_steps=50,
            save_strategy="no",
            output_dir="/root/autodl-tmp/output",
            fp16=True,  # å¼ºåˆ¶å¯ç”¨æ··åˆç²¾åº¦
            dataloader_num_workers=4,
            remove_unused_columns=False,
            report_to="none",
        )

        # ================= è‡ªå®šä¹‰æ•°æ®æ”¶é›†å™¨ =================
        def custom_collator(features):
            batch = {
                "input_ids": torch.stack([torch.tensor(f["input_ids"]) for f in features]),
                "attention_mask": torch.stack([torch.tensor(f["attention_mask"]) for f in features]),
                "labels": torch.tensor([f["labels"] for f in features]),
                "keyword_features": torch.tensor([f["keyword_features"] for f in features], dtype=torch.float32),
            }
            return batch

        # ================= è®­ç»ƒæ‰§è¡Œ =================
        print("ğŸš€ å¯åŠ¨è®­ç»ƒ...")
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=processed_dataset["train"],
            eval_dataset=processed_dataset["validation"],
            data_collator=custom_collator,
            compute_metrics=lambda p: {
                "accuracy": (p.predictions.argmax(-1) == p.label_ids).mean(),
                "f1": f1_score(p.label_ids, p.predictions.argmax(-1), average="weighted")
            },
            callbacks=[GradientMonitor(), EarlyStoppingCallback(target_loss=0.3)]
        )

        trainer.train()
```
&emsp;&emsp;åœ¨è®­ç»ƒå‚æ•°éƒ¨åˆ†,ç‰©ç†æ‰¹æ¬¡2+æ¢¯åº¦ç´¯ç§¯16çš„æ··åˆè®­ç»ƒæ¨¡å¼ï¼Œç›¸å½“äºä¼ ç»Ÿ32æ‰¹æ¬¡çš„æ•ˆæœï¼Œä½†æ˜¾å­˜å³°å€¼æ˜æ˜¾é™ä½ã€‚(32æ‰¹æ¬¡4090æ˜¾å¡æœ€é«˜20000çš„æ˜¾å­˜ç›´æ¥çˆ†æ‰äº†,ä¼˜åŒ–åä¹Ÿç”¨äº†18000å·¦å³çš„æ˜¾å­˜)\
&emsp;&emsp;å­¦ä¹ ç‡é‡‡ç”¨1.5e-4é…åˆä½™å¼¦é€€ç«ç­–ç•¥ï¼Œå‰20%è®­ç»ƒæ­¥è¿›è¡Œé¢„çƒ­ï¼Œå‡å°‘äº†Transformeræ¨¡å‹çš„æ¢¯åº¦æŒ¯è¡é—®é¢˜ã€‚\
&emsp;&emsp;AdamWä¼˜åŒ–å™¨çš„Î²2=0.95è®¾ç½®ï¼Œç‰¹åˆ«é€‚é…é‡‘èæ–‡æœ¬çš„é•¿å°¾åˆ†å¸ƒç‰¹æ€§ã€‚"
Transformeræ¨¡å‹ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå‚æ•°ä¼—å¤šï¼Œè®­ç»ƒæ—¶æ¢¯åº¦å¯èƒ½ä¼šå‡ºç°ä¸ç¨³å®šçš„æ³¢åŠ¨ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒå±‚ä¹‹é—´ã€‚æ¯”å¦‚ï¼Œæµ…å±‚å’Œæ·±å±‚çš„æ¢¯åº¦å¯èƒ½å·®å¼‚å¾ˆå¤§ï¼Œå¯¼è‡´ä¼˜åŒ–è¿‡ç¨‹ä¸­å‚æ•°æ›´æ–°å¹…åº¦ä¸ä¸€è‡´ï¼Œå‡ºç°éœ‡è¡ï¼Œå½±å“æ”¶æ•›ã€‚è¿™æ—¶å€™ï¼Œä¼˜åŒ–å™¨çš„é€‰æ‹©å°±å¾ˆé‡è¦ï¼ŒAdamWä½œä¸ºAdamçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œèƒ½æ›´å¥½åœ°å¤„ç†æƒé‡è¡°å‡ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚\
&emsp;&emsp;AdamWä¸­çš„Î²2å‚æ•°æ§åˆ¶çš„æ˜¯æ¢¯åº¦äºŒé˜¶çŸ©çš„æŒ‡æ•°è¡°å‡ç‡ã€‚é»˜è®¤Î²2é€šå¸¸æ˜¯0.999ï¼Œè¿™æ ·ä¼šè€ƒè™‘æ›´é•¿æ—¶é—´æ®µçš„æ¢¯åº¦å¹³æ–¹ï¼Œé€‚ç”¨äºå¹³ç¨³çš„æ¢¯åº¦ç¯å¢ƒã€‚ä½†åœ¨é‡‘èæ–‡æœ¬ä¸­ï¼Œæ•°æ®åˆ†å¸ƒé•¿å°¾ï¼Œå³å°‘æ•°ç±»åˆ«æ ·æœ¬å¤šï¼Œå¤šæ•°ç±»åˆ«æ ·æœ¬å°‘ï¼Œå¯¼è‡´æ¢¯åº¦å˜åŒ–å¤§ï¼Œå°¤å…¶æ˜¯å°æ ·æœ¬ç±»åˆ«æ¢¯åº¦å¯èƒ½çªç„¶å‡ºç°è¾ƒå¤§çš„å˜åŒ–ã€‚å¦‚æœÎ²2è®¾ç½®è¾ƒé«˜ï¼ŒäºŒé˜¶çŸ©ä¼°è®¡ä¼šè¿‡äºå¹³æ»‘ï¼Œæ— æ³•å¿«é€Ÿé€‚åº”è¿™äº›çªç„¶çš„å˜åŒ–ï¼Œå¯¼è‡´å‚æ•°æ›´æ–°ä¸å¤Ÿçµæ•ã€‚é™ä½Î²2åˆ°0.95ï¼Œå¯ä»¥è®©äºŒé˜¶çŸ©ä¼°è®¡æ›´å¿«åœ°ååº”æœ€è¿‘çš„æ¢¯åº¦å˜åŒ–ï¼Œä»è€Œæ›´çµæ´»åœ°è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¯¹é•¿å°¾æ•°æ®ä¸­çš„ç¨€ç–ä½†é‡è¦çš„ä¿¡å·ï¼ˆå¦‚ç½•è§ä½†å…³é”®çš„é‡‘èæœ¯è¯­ï¼‰æœ‰æ›´å¥½çš„é€‚åº”æ€§ã€‚

### æ¨¡å‹å…¶ä»–éƒ¨åˆ†
```python
# ================= å…³é”®è¯é…ç½® =================
class KeywordConfig:
    LABEL_KEYWORDS = {
        0: ["å‡ºå”®", "å‡æŒ", "æŠ›å”®", "äºæŸ", "ä¸‹è·Œ", "è¯„çº§ä¸‹è°ƒ", "å‡€æµå‡º", 
            "æ¸…ä»“", "å‡€äº", "è‚¡ä»·å¤§è·Œ", "è¥æ”¶ä¸åŠé¢„æœŸ", "è£æ’¤", "ç½šå•",
            "å‡ºå”®è‚¡æƒ", "å‡ºå”®è‚¡ç¥¨", "å‡æŒè‚¡æƒ", "å‡æŒè‚¡ä»½", "å…¬å¸å‡èµ„", "æŒè‚¡ä¸‹é™",
            "è·Œå¹…æ‰©å¤§", "å‡€åˆ©æ¶¦ä¸‹é™", "å¼€ç›˜å¤§è·Œ", "é¢„æ‘˜ç‰Œ", "è‚¡ä»·ä¸‹è·Œ", "è‚¡ä»·è·Œå¹…æ‰©å¤§",
            "GMV ä¸‹é™", "ä¸æŒ‰è®¡åˆ’ä¸Šå¸‚", "è‚¡ä»·èµ°å¼º", "æ”¶å…¥ä¸‹æ»‘", "ç›®æ ‡ä»·ä¸‹è°ƒ", "ä¸Šå¸‚æš‚ç¼“",
            "å–æ¶ˆèŒä½", "ç½šå•è½åœ°", "æµ·å¤–å€ºé‡ç»„", "ä½å¼€", "é«˜å¼€ä½èµ°", "é¢†è·Œ", "æµå‡º", "ä¸‹", "äº"],
        1: ["ä¹°å…¥", "å¢æŒ", "å›è´­", "ç›ˆåˆ©å¢é•¿", "å¢é•¿æé€Ÿ", "æˆ˜ç•¥åˆä½œ", 
            "è‚¡ä»·ä¸Šæ¶¨", "å¸‚å€¼è¶…è¶Š", "ä¸šç»©è¶…é¢„æœŸ", "æŠ•èµ„", "ç ”å‘", "å¸ƒå±€å¸‚åœº",
            "å‡€ä¹°å…¥", "åŠ ä»“", "è°ƒä»“ä¹°å›", "æŒä»“æ›å…‰å¢æŒ", "æŒè‚¡å¢åŠ ", "æ‰©å¤§å›è´­", "å¢åŠ å›è´­",
            "æŠ„åº•", "è‚¡ä»·æ‹‰å‡", "å‡€èµš", "è®¢å•å¢é•¿", "æ”¶å…¥å¢é•¿", "æ¿å—å¢é•¿", "è¥æ”¶å¢é•¿",
            "å‡€åˆ©æ¶¦å¢é•¿", "è´¢å­£æ”¶å…¥å¢é•¿", "æ¶ˆè´¹è€…çªç ´", "è´¢å¹´æ”¶å…¥", "é˜¿é‡Œäº‘ç›ˆåˆ©", "é—¨åº—ç°é‡‘æµ",
            "æˆäº¤é¢çªç ´", "åˆä½œ", "å…¥è‚¡", "æ”¶è´­å¹³å°", "å‘å¸ƒæ¨¡å‹", "å¼€æºæ¨¡å‹", "æ”¶è´­è¦çº¦",
            "å‘å¸ƒèŠ¯ç‰‡", "æ‰“é€ å¼€æ”¾äº‘", "ä¸Šçº¿å›½å®¶é¦†", "åŠ ç å¸‚åœº", "è®¾ç«‹ä¸šåŠ¡é›†å›¢", "æ·±åŒ–åˆä½œ",
            "èµ„äº§æ³¨å…¥", "åŒæŸœå°è¯åˆ¸", "ç»´æŒè¯„çº§", "ç”³è¯·ä¸Šå¸‚", "æœ€ä½³è´¢åŠ¡çŠ¶å†µ", "å¼€æºå¼€æ”¾è½¬åŒ–ç”Ÿäº§åŠ›",
            "å¤©çŒ«åŒ 11", "æ•°å­—åŒ–", "æ‰æ ¹å®ä½“ç»æµ", "å¤©çŒ«åŒ 11 æŠ¥å‘Š", "è‚¡ä»·é¢†æ¶¨", "æ¶¨å¹…æ‰©å¤§",
            "è‚¡ä»·æ”¶æ¶¨", "è‚¡ä»·æ¶¨å¹…", "è½¯é“¶è¾Ÿè°£è‚¡ä»·ä¸Šæ¶¨", "å…¬ç›Šæèµ ", "æ”¶è´­", "æ”¶è´­å…¬å¸"],
        2: ["å‘è¡Œç¥¨æ®", "æˆç«‹åŸºé‡‘", "ç®¡ç†äº¤æ¥", "è´¢æŠ¥å‘å¸ƒ", 
            "ç»„ç»‡æ¶æ„è°ƒæ•´", "å›åº”è¯¢é—®", "å¸¸è§„å…¬å‘Š", "äººäº‹å˜åŠ¨", "å¸‚åœºå±•æœ›",
            "å‘è¡Œå€ºåˆ¸", "å¤§å®—äº¤æ˜“", "å­å…¬å¸æ‹›è˜", "åˆ†ç«‹æ‰¿ç»§æŒè‚¡æœªå˜", "ç”³è¯·å•†æ ‡", "æ³¨èµ„",
            "æ‰¿åŠæ´»åŠ¨", "ç”³è¯· IPO", "åˆ†æ‹†ä¸Šå¸‚", "SPAC è†è®¯", "å‰é«˜ç®¡", "é©¬äº‘å›å›½",
            "æˆç«‹å®éªŒå®¤", "åŠ©å†œ", "å®šåˆ¶ç”µæº", "å»ºè®¾åŸºç¡€è®¾æ–½", "å…ƒå®‡å®™å¤§ä¼š", "è‡ªæ„¿è½¬æ¢",
            "æå‰ç»“ç®—", "ç”³è¯·æŸœå°", "SEC è¯„ä¼°", "è´¢æŠ¥", "å‘å¸ƒæŠ¥å‘Š", "åˆä½œä¼™ä¼´å˜åŠ¨",
            "å…³è”äº¤æ˜“", "é€€ä¼‘å±¥æ–°èŒ", "æåè‘£äº‹æ¨è¿›æˆ˜ç•¥", "åŠ ç›Ÿæ¨è¿›", "æ•´åˆæˆç«‹å‚ä¸å¢èµ„",
            "æ¢å¸…", "ç»„ç»‡æ¶æ„å˜é©", "CEO è°ˆå®šä½", "çœå§”ä¹¦è®°åº§è°ˆ", "æˆç«‹å­å…¬å¸", "æ— ç›´æ¥å…³è”",
            "ç°èº«", "å›åº”ç€ç«", "ç›®æ ‡ä»·", "ä¿æŒä¸Šå¸‚åœ°ä½", "å›åº”å‹åˆ¶åå¥½", "å‰”é™¤å…³æ³¨åå•",
            "è¥æ”¶è¶…é¢„æœŸç»´æŒè¯„çº§", "å‘å¸ƒä¸šç»©", "å±•æœ›ä¸Šè°ƒ", "å…¬å¸ƒä¸šç»©", "å­£æŠ¥å‘å£°",
            "è‚¡ä¸œå¤§ä¼šä¸»è¦ä¸Šå¸‚", "å­£æŠ¥", "é‡ç”³è¯„çº§", "å‘å¸ƒå·¥å…·"]
    }
# ================= æç¤ºè¯é…ç½® =================
class PromptConfig:
    PROMPT_TEMPLATES = [
        "ä½œä¸ºé‡‘èåˆ†æå¸ˆï¼Œåˆ¤æ–­ä»¥ä¸‹æ¨æ–‡çš„æƒ…æ„Ÿå€¾å‘ï¼ˆçœ‹è·Œã€çœ‹æ¶¨ã€ä¸­æ€§ï¼‰ï¼š\n{}",
        "è¯·åˆ†ææ–‡æœ¬ä¸­çš„å¸‚åœºæƒ…ç»ªï¼ˆé€‰é¡¹ï¼šçœ‹è·Œã€çœ‹æ¶¨ã€ä¸­æ€§ï¼‰ï¼š\n{}",
        "[é‡‘èæƒ…æ„Ÿåˆ†æ] æ–‡æœ¬å†…å®¹ï¼š{}\nè¯·é€‰æ‹©æœ€åˆé€‚çš„æƒ…æ„Ÿæ ‡ç­¾ï¼š",
        "æ ¹æ®ä»¥ä¸‹æ–‡æœ¬åˆ¤æ–­å¸‚åœºæƒ…ç»ªï¼š\n{}\né€‰é¡¹ï¼šçœ‹è·Œ | çœ‹æ¶¨ | ä¸­æ€§\nå›ç­”ï¼š",
        "å¸‚åœºæƒ…ç»ªåˆ†æä»»åŠ¡ï¼š\nè¾“å…¥æ–‡æœ¬ï¼š{}\nè¾“å‡ºåˆ†ç±»ç»“æœï¼š"
    ]
    
    @classmethod
    def apply_prompt(cls, text, eval_mode=False):
        import random
        template = cls.PROMPT_TEMPLATES[0] if eval_mode else random.choice(cls.PROMPT_TEMPLATES)
        return template.format(text.strip())
# ================= å¼ºåŒ–æ•°æ®å¤„ç†å™¨ =================
class EnhancedDataProcessor:
    @staticmethod
    def text_augmentation(text):
        replacements = {
            r'\$': '',
            r'https?://\S+': '',
            r'[^\w\s.,!?%$()/-]': ' ',
            r'\s+': ' ',
        }
        for pat, repl in replacements.items():
            text = re.sub(pat, repl, text)
        return text.strip()[:400]

    @classmethod
    def extract_keyword_features(cls, text):
        features = [0] * 3
        for label, keywords in KeywordConfig.LABEL_KEYWORDS.items():
            # ä¸­æ–‡å…³é”®è¯åŒ¹é…ä¼˜åŒ–
            features[label] = sum(
                len(re.findall(r'{}'.format(re.escape(kw)), text))
                for kw in keywords
            )
        return [min(3, f)**2 for f in features]

    @classmethod
    def process_batch(cls, examples):
        processed = {"text": [], "label": [], "keyword_features": []}
        error_log = []
        label_counts = {0:0, 1:0, 2:0}

        for idx, (text, raw_label) in enumerate(zip(examples["text"], examples["label"])):
            try:
                label = LabelSystem.to_int(raw_label)
                clean_text = cls.text_augmentation(text)
                
                if len(clean_text) < 2:
                    raise ValueError("æ–‡æœ¬è¿‡çŸ­")

                prompted_text = PromptConfig.apply_prompt(clean_text)
                features = cls.extract_keyword_features(clean_text)
                
                processed["text"].append(prompted_text)
                processed["label"].append(label)
                processed["keyword_features"].append(features)
                label_counts[label] += 1

            except Exception as e:
                error_log.append(f"è¡Œ {idx+1}: {str(e)} | æ–‡æœ¬: {text[:40]}...")

        max_count = max(label_counts.values())
        for lbl, count in label_counts.items():
            if count < max_count//2:
                duplicated = [
                    (p, f) for p, l, f in zip(processed["text"], processed["label"], processed["keyword_features"])
                    if l == lbl
                ][:max_count//2 - count]
                processed["text"].extend([d[0] for d in duplicated])
                processed["label"].extend([lbl]*len(duplicated))
                processed["keyword_features"].extend([d[1] for d in duplicated])

        if error_log:
            print(f"âš ï¸ è¿‡æ»¤ {len(error_log)} ä¸ªæ— æ•ˆæ ·æœ¬")

        return processed
```
&emsp;&emsp;ç¬¬ä¸€æ®µçœ‹ä¸Šå»éå¸¸åƒå…³é”®è¯åŒ¹é…,å®é™…ä¸Šå…¶å®ä¹Ÿæ˜¯å…³é”®è¯åŒ¹é…(å…³é”®è¯åŒ¹é…çœŸçš„å¾ˆå¥½ç”¨qwq,æœ€åˆçš„æ¨¡å‹æ²¡æœ‰è¿™éƒ¨åˆ†å‡†ç¡®ç‡åªæœ‰0.7,åŠ ä¸Šä¹‹åç›´å¥”0.95),æ€ä¹ˆè¯´å‘¢,å®é™…ä¸Šè¿™ä¸€éƒ¨åˆ†åº”è¯¥æ¢æˆå…¶ä»–çš„æ¨¡å¼,æ¯”å¦‚é‡‘èçŸ¥è¯†å›¾è°±,æŠŠçŸ¥è¯†å–‚ç»™å¤§æ¨¡å‹å‘Šè¯‰å®ƒä»€ä¹ˆæ˜¯æ­£é¢çš„ä»€ä¹ˆæ˜¯ä¸­æ€§çš„ã€‚ä½†æ˜¯æˆ‘å°‘äº†è¿™ä¸€éƒ¨åˆ†,å°½ç®¡æ¥è¯´,è¾¾åˆ°çš„æ˜¯ä¸€æ ·çš„æ•ˆæœ,æœ‰ä¸€ç‚¹ç‚¹å·å·¥å‡æ–™(bushi)\
&emsp;&emsp;ç¬¬äºŒæ®µæ˜¯promptå·¥ç¨‹;ç¬¬ä¸‰æ®µæ˜¯æ–°é—»é¢„å¤„ç†(ä¸€å¼€å§‹æˆ‘ä½¿ç”¨çš„æ˜¯huggingfaceä¸Šçš„twitterè´¢ç»æ–°é—»æ•°æ®é›†,æœ‰æ ‡æ³¨çš„,é‡Œé¢æœ‰å¾ˆå¤šå…¶ä»–çš„å¥‡å¥‡æ€ªæ€ªçš„ç¬¦å·,åæ¥æ¢æˆçˆ¬å–çš„ä¸­æ–‡æ•°æ®é›†ç”¨ä¸ä¸Šäº†,ä¸è¿‡æˆ‘æ²¡åˆ å€’æ˜¯)\
æ¯æ—¥åˆ†æ•°è®¡ç®—:$daily\_score = \frac{æ­£é¢æ–°é—»æ•°é‡-è´Ÿé¢æ–°é—»æ•°é‡}{æ€»æ•°é‡}$\
æœ€åè¾“å‡ºæ¯å¤©çš„æƒ…æ„Ÿåˆ†æ•°,åˆ°è¿™é‡Œ,é‡‘èæƒ…æ„Ÿåˆ†ææ¨¡å‹éƒ¨åˆ†ç®—æ˜¯ç»“æŸäº†

## æ¯æ—¥ç»¼åˆé£é™©åˆ†æ•°è®¡ç®—
### è´¢åŠ¡æŠ¥è¡¨å¾—åˆ†Altman-z-score
ç»å…¸è®¡ç®—æ–¹å¼:$Z = 0.717 * X1 + 0.847 * X2 + 3.107 * X3 + 0.42 * X4 + 0.998 * X5$

### è¡°å‡æ¯æ—¥æƒ…æ„Ÿå¾—åˆ†
&emsp;&emsp;å¼•å…¥æŒ‡æ•°è¡°å‡æœºåˆ¶æ¨¡æ‹Ÿå¸‚åœºè®°å¿†è§„å¾‹â€”â€”ä»¥0.7ä¸ºè¡°å‡å› å­ï¼Œ5å¤©ä¸ºæ—¶é—´çª—å£æ„å»ºæƒ…æ„Ÿæ—¶é—´é€é•œã€‚è®¡ç®—å…¬å¼ä¸­ï¼Œç¬¬t-nå¤©æƒ…æ„Ÿå€¼æƒé‡ä¸º0.7â¿ï¼Œä½¿äº”å¤©å‰çš„æƒ…ç»ªå½±å“ä»…ä¸ºå½“å‰çš„1.6%ï¼Œå®Œç¾ä½“ç°è¡Œä¸ºé‡‘èå­¦çš„è¿‘å› æ•ˆåº”ã€‚
> Bollen J, Mao H, Zeng X. Twitter mood predicts the stock market[J]. Journal of computational science, 2011, 2(1): 1-8.

```python
# æƒ…æ„Ÿåˆ†æ•°è¡°å‡è®¡ç®—
def calculate_decayed_sentiment(df, tau=30):
    """è®¡ç®—æ—¶é—´è¡°å‡åçš„æƒ…æ„Ÿåˆ†æ•°"""
    decay_factor = math.exp(-1/tau)
    
    for company in COMPANY_MAP.values():
        sum_num = 0.0
        sum_den = 0.0
        decayed_values = []
        
        for idx in range(len(df)):
            current_s = df.at[idx, company]
            
            # é€’æ¨è®¡ç®—
            sum_num = current_s + decay_factor * sum_num
            sum_den = 1 + decay_factor * sum_den
            decayed = sum_num / sum_den if sum_den > 1e-6 else 0.0
            
            decayed_values.append(round(decayed, 4))
        
        df[f'{company}_S'] = decayed_values
    return df
```

###  è®¡ç®—ç»¼åˆåˆ†æ•°
ç»¼åˆåˆ†æ•°å°±æ˜¯ç®€å•çš„çº¿æ€§è®¡ç®—,è®©ç»“æ„åŒ–åˆ†æ•°ä¸ºä¸»,æ–°é—»èˆ†æƒ…ä¸ºè¾…
```python
results = []
    for idx in range(len(sentiment_df)):
        date = sentiment_df.at[idx, 'date'].date()
        row_data = {'date': date.strftime('%Y-%m-%d')}
        
        for company in COMPANY_MAP.values():
            # è·å–è¡°å‡åæƒ…æ„Ÿåˆ†æ•°
            s_prime = sentiment_df.at[idx, f'{company}_S']
            
            # è·å–Z-Score
            z = get_zscore(company_z[company], date)
            
            # ç»¼åˆåˆ†æ•°è®¡ç®—ï¼ˆ0.7*z + 0.3*s_primeï¼‰
            composite = 0.7 * z + 0.3 * s_prime
            row_data[company] = round(composite, 4)  # å­˜å‚¨ä¸ºå•ä¸ªæµ®ç‚¹æ•°
            
        results.append(row_data)
    
```
## å¯è§†åŒ–ç½‘ç«™
&emsp;&emsp;å¯è§†åŒ–ç½‘ç«™é‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œå‰ç«¯åŸºäºVue.jså®ç°å“åº”å¼äº¤äº’ï¼Œåç«¯ä¾æ‰˜Flaskæ¡†æ¶æä¾›RESTful APIæœåŠ¡
&emsp;&emsp;æœ€åå®Œæˆæˆªå›¾like:
![](/post/financial-risk-LLM/å¯è§†åŒ–ç½‘ç«™.png)

### å‰ç«¯
#### ä»ªè¡¨æ¿åˆå§‹åŒ–
```js
// åˆå§‹åŒ–é¡µé¢
document.addEventListener('DOMContentLoaded', () => {
    // è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æ•°æ®ï¼‰
    const today = new Date();
    // è®¾ç½®ç»“æŸæ—¥æœŸä¸ºä»Šå¤©
    document.getElementById('end-date').value = formatDate(today);
    
    // ä¸è®¾ç½®å¼€å§‹æ—¥æœŸçš„å€¼ï¼Œè®©APIè¿”å›æ‰€æœ‰å†å²æ•°æ®
    // è¿™æ ·å¯ä»¥ç¡®ä¿æ˜¾ç¤ºä»æœ€æ—©è®°å½•åˆ°ä»Šå¤©çš„æ‰€æœ‰æ•°æ®
    document.getElementById('start-date').value = '';
    
    // ç»‘å®šæŒ‰é’®äº‹ä»¶
    document.getElementById('apply-filters').addEventListener('click', fetchAndUpdateData);
    
    // æ·»åŠ å…¬å¸å¤é€‰æ¡†çš„å¿«é€Ÿé€‰æ‹©/å–æ¶ˆé€‰æ‹©åŠŸèƒ½
    const checkboxes = document.querySelectorAll('.checkbox-group input[type="checkbox"]');
    checkboxes.forEach(checkbox => {
        checkbox.addEventListener('change', () => {
            // å¦‚æœç”¨æˆ·æ‰‹åŠ¨æ›´æ”¹äº†é€‰æ‹©ï¼Œå¯ä»¥ç«‹å³æ›´æ–°å›¾è¡¨
            if (document.querySelectorAll('.checkbox-group input[type="checkbox"]:checked').length > 0) {
                fetchAndUpdateData();
            }
        });
    });
    
    // åˆå§‹åŠ è½½æ•°æ®
    console.log('Initializing chart...');
    fetchAndUpdateData();
});
```
åˆå§‹åŒ–é¡µé¢åŠ è½½,è®¾ç½®é»˜è®¤æ—¶é—´èŒƒå›´ä¸º"æ‰€æœ‰å†å²æ•°æ®",ç»‘å®šç­›é€‰æŒ‰é’®å’Œå¤é€‰æ¡†çš„äº¤äº’äº‹ä»¶,é¦–æ¬¡åŠ è½½æ•°æ®è§¦å‘æ•´ä¸ªåº”ç”¨å¯åŠ¨

#### æ•°æ®è·å–
```js
// è·å–å¹¶æ›´æ–°æ•°æ®
async function fetchAndUpdateData() {
    try {
        // è·å–ç­›é€‰æ¡ä»¶
        const startDate = document.getElementById('start-date').value;
        const endDate = document.getElementById('end-date').value;
        
        // è·å–é€‰ä¸­çš„å…¬å¸
        const selectedCompanies = [];
        document.querySelectorAll('.checkbox-group input[type="checkbox"]:checked').forEach(checkbox => {
            selectedCompanies.push(checkbox.value);
        });
        
        if (selectedCompanies.length === 0) {
            alert('è¯·è‡³å°‘é€‰æ‹©ä¸€å®¶å…¬å¸');
            return;
        }
        
        // è·å–é€‰ä¸­çš„æƒ…æ„ŸæŒ‡æ ‡ç±»å‹
        const sentimentType = document.getElementById('sentiment-type').value;
        
        // æ„å»ºAPI URL
        let url = '/api/sentiment-data';
        const params = new URLSearchParams();
        
        if (startDate) params.append('start_date', startDate);
        if (endDate) params.append('end_date', endDate);
        if (selectedCompanies.length > 0) params.append('companies', selectedCompanies.join(','));
        params.append('sentiment_type', sentimentType);
        
        const fullUrl = `${url}?${params.toString()}`;
        console.log('Fetching data from:', fullUrl);
        
        // æ˜¾ç¤ºåŠ è½½çŠ¶æ€
        document.querySelector('.chart-container').innerHTML = `
<div class="loading-container">
  <div class="loading-spinner"></div>
  <div class="loading-text">æ•°æ®åŠ è½½ä¸­...</div>
</div>
`;
        
        // è·å–æ•°æ®
        const response = await fetch(fullUrl);
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const data = await response.json();
        console.log('Received data:', data);
        
        if (data.error) {
            throw new Error(data.error);
        }
        
        if (!Array.isArray(data) || data.length === 0) {
            document.querySelector('.chart-container').innerHTML = '<div class="error">æ‰€é€‰æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®</div>';
            return;
        }
        
        allData = data;
        
        // æ›´æ–°å›¾è¡¨
        updateChart(data, selectedCompanies);
        
        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        updateStats(data, selectedCompanies);
        
    } catch (error) {
        console.error('Error fetching data:', error);
        document.querySelector('.chart-container').innerHTML = `<div class="error">åŠ è½½æ•°æ®å¤±è´¥: ${error.message}</div>`;
    }
}
```
ä»æŒ‡å®šä½ç½®è·å–æ‰€æœ‰æ•°æ®,æ”¯æŒæ—¥æœŸèŒƒå›´/å…¬å¸é€‰æ‹©/æŒ‡æ ‡ç±»å‹å¤šç»´è¿‡æ»¤

#### æ›´æ–°å›¾è¡¨
```js
// æ›´æ–°å›¾è¡¨
function updateChart(data, selectedCompanies) {
    // å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
    if (data.length === 0) {
        document.querySelector('.chart-container').innerHTML = '<div class="error">æ‰€é€‰æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®</div>';
        return;
    }
    
    // è·å–å½“å‰é€‰æ‹©çš„æƒ…æ„ŸæŒ‡æ ‡ç±»å‹
    const sentimentType = document.getElementById('sentiment-type').value;
    
    // å‡†å¤‡å›¾è¡¨æ•°æ®
    const chartData = {
        datasets: []
    };
    
    // ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
    data.sort((a, b) => new Date(a.date) - new Date(b.date));
    
    // ä¸ºæ¯ä¸ªé€‰ä¸­çš„å…¬å¸åˆ›å»ºä¸€ä¸ªæ•°æ®é›†
    selectedCompanies.forEach(company => {
        const companyData = data.map(item => {
            // æ£€æŸ¥è¯¥å…¬å¸çš„æ•°æ®æ˜¯å¦å­˜åœ¨
            if (item[company] === undefined) {
                console.log(`Missing data for ${company} on ${item.date}`);
                return null;
            }
            return {
                x: new Date(item.date),
                y: parseFloat(item[company])
            };
        }).filter(point => point !== null && !isNaN(point.y)); // è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®
        
        // å¦‚æœè¯¥å…¬å¸æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡
        if (companyData.length === 0) {
            console.log(`No valid data for ${company}`);
            return;
        }
        
        chartData.datasets.push({
            label: company,
            data: companyData,
            borderColor: companyColors[company],
            backgroundColor: `${companyColors[company]}33`, // æ·»åŠ é€æ˜åº¦
            borderWidth: 2,
            pointRadius: 3,
            pointHoverRadius: 5,
            tension: 0.1, // ä½¿çº¿æ¡æ›´å¹³æ»‘
            fill: false
        });
    });
    
    // å¦‚æœæ‰€æœ‰å…¬å¸éƒ½æ²¡æœ‰æœ‰æ•ˆæ•°æ®
    if (chartData.datasets.length === 0) {
        document.querySelector('.chart-container').innerHTML = '<div class="error">æ‰€é€‰å…¬å¸åœ¨æ­¤æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®</div>';
        return;
    }
    
    // é”€æ¯æ—§å›¾è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if (sentimentChart) {
        sentimentChart.destroy();
    }
    
    // åˆ›å»ºå›¾è¡¨å®¹å™¨
    document.querySelector('.chart-container').innerHTML = '<canvas id="sentiment-chart"></canvas>';
    
    // è·å–Yè½´èŒƒå›´
    const yAxisRange = getYAxisRangeBySentimentType(sentimentType);
    
    // åˆ›å»ºæ–°å›¾è¡¨
    const ctx = document.getElementById('sentiment-chart').getContext('2d');
    sentimentChart = new Chart(ctx, {
        type: 'line',
        data: chartData,
        options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
                x: {
                    type: 'time',
                    time: {
                        unit: 'day',
                        displayFormats: {
                            day: 'yyyy-MM-dd'
                        },
                        tooltipFormat: 'yyyy-MM-dd'
                    },
                    title: {
                        display: true,
                        text: 'æ—¥æœŸ',
                        font: {
                            weight: 'bold'
                        }
                    },
                    grid: {
                        display: true,
                        color: 'rgba(0, 0, 0, 0.05)'
                    }
                },
                y: {
                    min: yAxisRange.min,
                    max: yAxisRange.max,
                    title: {
                        display: true,
                        text: 'æƒ…æ„Ÿåˆ†æ•°',
                        font: {
                            weight: 'bold'
                        }
                    },
                    grid: {
                        display: true,
                        color: 'rgba(0, 0, 0, 0.05)'
                    },
                    ticks: {
                        callback: function(value) {
                            return value.toFixed(1);
                        }
                    }
                }
            },
            plugins: {
                zoom: {
                    zoom: {
                        wheel: {
                            enabled: true,
                        },
                        pinch: {
                            enabled: true
                        },
                        mode: 'xy',
                    },
                    pan: {
                        enabled: true,
                        mode: 'xy',
                    }
                },
                tooltip: {
                    mode: 'index',
                    intersect: false,
                    backgroundColor: 'rgba(0, 0, 0, 0.7)',
                    padding: 10,
                    cornerRadius: 4,
                    callbacks: {
                        title: function(tooltipItems) {
                            return new Date(tooltipItems[0].parsed.x).toLocaleDateString('zh-CN');
                        },
                        label: function(context) {
                            let label = context.dataset.label || '';
                            if (label) {
                                label += ': ';
                            }
                            if (context.parsed.y !== null) {
                                label += context.parsed.y.toFixed(2);
                                // æ·»åŠ æƒ…æ„Ÿæè¿°
                                if (context.parsed.y > 0.15) label += ' (å¼ºæ­£é¢)';
                                else if (context.parsed.y > 0) label += ' (å¼±æ­£é¢)';
                                else if (context.parsed.y < -0.5) label += ' (å¼ºè´Ÿé¢)';
                                else if (context.parsed.y < 0) label += ' (å¼±è´Ÿé¢)';
                                else label += ' (ä¸­æ€§)';
                            }
                            return label;
                        }
                    }
                },
                legend: {
                    position: 'top',
                    labels: {
                        usePointStyle: true,
                        padding: 20,
                        font: {
                            size: 12
                        }
                    }
                },
                title: {
                    display: true,
                    text: getTitleBySentimentType(sentimentType),
                    font: {
                        size: 16,
                        weight: 'bold'
                    },
                    padding: {
                        top: 10,
                        bottom: 20
                    },
                    color: '#2c3e50'
                }
            },
            interaction: {
                mode: 'nearest',
                axis: 'x',
                intersect: false
            },
            animation: {
                duration: 1000,
                easing: 'easeOutQuart'
            },
            onClick: handleChartClick // æ·»åŠ ç‚¹å‡»äº‹ä»¶å¤„ç†
        }
    });
}
```
&emsp;&emsp;è¿™æ®µä»£ç çš„ç›®çš„æ˜¯æ›´æ–°å¹¶æ¸²æŸ“ä¸€ä¸ªå›¾è¡¨ï¼Œä»£ç ä»é¡µé¢ä¸­è·å–ç”¨æˆ·é€‰æ‹©çš„æƒ…æ„ŸæŒ‡æ ‡ç±»å‹ï¼Œç„¶ååˆå§‹åŒ–ä¸€ä¸ªç©ºçš„æ•°æ®é›†ï¼Œç”¨æ¥å­˜å‚¨å›¾è¡¨éœ€è¦çš„å„ä¸ªå…¬å¸çš„æ•°æ®ã€‚åœ¨å‡†å¤‡æ•°æ®çš„è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆä¼šå¯¹æ•°æ®è¿›è¡ŒæŒ‰æ—¥æœŸæ’åºï¼Œç¡®ä¿ç»˜åˆ¶çš„å›¾è¡¨æŒ‰æ—¶é—´é¡ºåºæ˜¾ç¤ºã€‚\
&emsp;&emsp;å¯¹äºæ¯ä¸ªé€‰ä¸­çš„å…¬å¸ï¼Œä»£ç ä¼šéå†åŸå§‹æ•°æ®å¹¶æå–å‡ºè¯¥å…¬å¸å¯¹åº”çš„æƒ…æ„Ÿåˆ†æ•°ã€‚å¦‚æœè¯¥å…¬å¸åœ¨æŸäº›æ—¥æœŸæ²¡æœ‰æ•°æ®ï¼Œä»£ç ä¼šè·³è¿‡è¿™äº›æ•°æ®ç‚¹ï¼ŒåŒæ—¶åœ¨æ§åˆ¶å°è¾“å‡ºç¼ºå¤±æ•°æ®çš„æé†’ã€‚åªè¦è¯¥å…¬å¸æœ‰æœ‰æ•ˆæ•°æ®ï¼Œå°±ä¼šå°†å…¶åŠ å…¥åˆ°å›¾è¡¨çš„æ•°æ®é›†ä¸­ã€‚ã€‚\
&emsp;&emsp;ä½¿ç”¨ Chart.js åˆ›å»ºä¸€ä¸ªæ–°çš„æŠ˜çº¿å›¾ï¼Œå¹¶é€šè¿‡å„ç§é…ç½®é€‰é¡¹æ¥å®šåˆ¶å›¾è¡¨çš„è¡Œä¸ºï¼Œæ¯”å¦‚Xè½´å’ŒYè½´çš„æ ‡ç­¾ã€æ˜¾ç¤ºæ ¼å¼ã€ç½‘æ ¼ã€åˆ»åº¦ã€å›¾ä¾‹ã€æ ‡é¢˜ã€å·¥å…·æç¤ºç­‰ã€‚\
&emsp;&emsp;tooltipæ’ä»¶è¢«ç”¨æ¥æ˜¾ç¤ºè¯¦ç»†çš„æƒ…æ„Ÿåˆ†æ•°ï¼Œå½“é¼ æ ‡æ‚¬æµ®åœ¨å›¾è¡¨ä¸Šæ—¶ï¼Œé™¤äº†æ˜¾ç¤ºåˆ†æ•°ï¼Œè¿˜ä¼šæ ¹æ®æƒ…æ„Ÿåˆ†æ•°çš„å¤§å°æ·»åŠ ä¸€äº›æè¿°ï¼Œå¦‚â€œå¼ºæ­£é¢â€ã€â€œå¼±è´Ÿé¢â€ç­‰

#### ç»Ÿè®¡å¡ç‰‡
```js
// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
function updateStats(data, selectedCompanies) {
    const statsGrid = document.getElementById('stats-grid');
    statsGrid.innerHTML = '';
    
    selectedCompanies.forEach(company => {
        // æå–è¯¥å…¬å¸çš„æ‰€æœ‰æ•°æ®
        const companyData = data.map(item => {
            if (item[company] === undefined) return NaN;
            return parseFloat(item[company]);
        });
        
        // è¿‡æ»¤æ‰NaNå€¼
        const validData = companyData.filter(value => !isNaN(value));
        
        if (validData.length === 0) {
            // å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œæ˜¾ç¤ºæ— æ•°æ®ä¿¡æ¯
            const statCard = document.createElement('div');
            statCard.className = `stat-card ${getCompanyClass(company)}`;
            statCard.innerHTML = `
                <h3>${company}</h3>
                <div class="stat-value">æš‚æ— æ•°æ®</div>
            `;
            statsGrid.appendChild(statCard);
            return;
        }
        
        // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        const average = validData.reduce((sum, value) => sum + value, 0) / validData.length;
        const max = Math.max(...validData);
        const min = Math.min(...validData);
        
        // è®¡ç®—æ­£é¢ã€è´Ÿé¢ä¸­å’Œæ€§çš„å¤©æ•°
        const positiveDays = validData.filter(value => value > 0).length;
        const negativeDays = validData.filter(value => value < 0).length;
        const neutralDays = validData.filter(value => value === 0).length;
        
        // åˆ›å»ºç»Ÿè®¡å¡ç‰‡
        const statCard = document.createElement('div');
        statCard.className = `stat-card ${getCompanyClass(company)}`;
        statCard.innerHTML = `
            <h3>${company}</h3>
            <div class="stat-value">å¹³å‡æƒ…æ„Ÿåˆ†æ•°: ${average.toFixed(2)}</div>
            <div class="stat-value">æœ€é«˜åˆ†æ•°: ${max.toFixed(2)}</div>
            <div class="stat-value">æœ€ä½åˆ†æ•°: ${min.toFixed(2)}</div>
            <div class="stat-value">æ­£é¢å¤©æ•°: ${positiveDays}</div>
            <div class="stat-value">è´Ÿé¢å¤©æ•°: ${negativeDays}</div>
            <div class="stat-value">ä¸­æ€§å¤©æ•°: ${neutralDays}</div>
            <div class="stat-value">æ€»å¤©æ•°: ${validData.length}</div>
        `;
        
        statsGrid.appendChild(statCard);
    });
}
```
&emsp;&emsp;è®¡ç®—ä»£ç ä¸­çš„ä¿¡æ¯å¹¶åœ¨ç½‘ç«™æœ€ä¸‹é¢è´´å‡ºç»Ÿè®¡ä¿¡æ¯

### åç«¯
```python
def get_composite_data(start_date, end_date, companies, sentiment_type):
    """è·å–ç»¼åˆæ•°æ®"""
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(COMPOSITE_DATA_PATH)
        
        # å°†æ—¥æœŸåˆ—è®¾ç½®ä¸ºæ—¥æœŸæ—¶é—´æ ¼å¼
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        # è¿‡æ»¤æ•°æ® - ç¡®ä¿æ—¥æœŸå‚æ•°ä¹Ÿè½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´æ ¼å¼
        if start_date:
            start_date = pd.to_datetime(start_date, errors='coerce')
            df = df[df['date'] >= start_date]
        if end_date:
            end_date = pd.to_datetime(end_date, errors='coerce')
            df = df[df['date'] <= end_date]
        
        # é‡æ–°ç»„ç»‡æ•°æ®ç»“æ„
        result_data = []
        
        # è·å–å”¯ä¸€æ—¥æœŸåˆ—è¡¨
        dates = df['date'].dt.strftime('%Y-%m-%d').unique()
        
        for date in dates:
            date_data = {'date': date}
            # è·å–å½“å¤©çš„æ•°æ®
            day_data = df[df['date'].dt.strftime('%Y-%m-%d') == date]
            
            # å¦‚æœæŒ‡å®šäº†å…¬å¸ï¼Œåªå¤„ç†è¿™äº›å…¬å¸çš„æ•°æ®
            if companies:
                companies_list = companies.split(',')
                for company in companies_list:
                    if company in day_data.columns:
                        date_data[company] = float(day_data[company].values[0]) if not pd.isna(day_data[company].values[0]) else 0.0
            else:
                # å¤„ç†æ‰€æœ‰å…¬å¸çš„æ•°æ®
                for company in ['è…¾è®¯', 'ç™¾åº¦', 'ç½‘æ˜“', 'é˜¿é‡Œå·´å·´']:
                    if company in day_data.columns:
                        date_data[company] = float(day_data[company].values[0]) if not pd.isna(day_data[company].values[0]) else 0.0
            
            result_data.append(date_data)
        
        # æŒ‰æ—¥æœŸæ’åº
        result_data.sort(key=lambda x: x['date'])
        
        return jsonify(result_data)
    except Exception as e:
        print(f"Error in get_composite_data: {str(e)}")
        print(traceback.format_exc())
        return jsonify({'error': str(e)}), 500
```
&emsp;&emsp;åç«¯æ˜¯å››ä¸ªç›¸ä¼¼çš„æ¨é€æ•°æ®çš„ä»£ç ,é€‰å–äº†ä¸€ä¸ªæ¨é€ç»¼åˆåˆ†æ•°çš„ä½œä¸ºä»£è¡¨\
&emsp;&emsp;æ€»çš„æ¥è¯´æ˜¯è¯»å–å„ä¸ªç»´åº¦çš„csvæ–‡ä»¶,é’ˆå¯¹æ–‡ä»¶æ•°æ®å’Œæ ¼å¼é‡‡ç”¨ä¸åŒçš„æ–¹å¼å¤„ç†å,

## æ€»ç»“
&emsp;&emsp;æœ¬æ–‡ä¹Ÿç®—æ˜¯ç¬¬ä¸€æ‰¹åšä¸­æ–‡å¤§æ¨¡å‹é¢„æµ‹é‡‘èé£é™©åˆ†æçš„äº†,å¼€é¢˜å½“æ—¶ç½‘ä¸Šè¿˜æ²¡æœ‰ç›¸å…³çš„æ•°æ®å’Œæ–°é—»æŠ¥é“(é™¤äº†23å¹´çš„æ‹›è”æå¾—æ‹›è”æ™ºé¹¿)ã€‚åœ¨æ–‡æœ¬ä¸­æœŸæ—¶ä¸­æ–‡å¤§æ¨¡å‹é¢„æµ‹é‡‘èé£é™©ä¾¿å¦‚é›¨åæ˜¥ç¬‹,25å¹´å·²ç»æœ‰å¾ˆå¤šè¾ƒä¸ºæˆç†Ÿçš„äº§å“äº†ã€‚
&emsp;&emsp;æœ¬æ–‡ä»…ä»…åœ¨æ–°é—»èˆ†è®ºéƒ¨åˆ†å–å¾—äº†åŠæ ¼çš„ç»“æœ(å‡†ç¡®ç‡:94.8%,å¬å›ç‡:94.7%,F1-score:94.7%)æ•´ä½“ä»æœ‰è®¸å¤šä¸è¶³ä¹‹å¤„,åœ¨æ–°é—»æƒ…æ„Ÿåˆ†æéƒ¨åˆ†åº”è¯¥æ·»åŠ è¯†åˆ«æ–°é—»ä¸»ä½“çš„ç®—æ³•;å…¬å¸è´¢æŠ¥ä»»ä¸èƒ½å®æ—¶çš„å±•ç°ç»“æ„åŒ–çš„é£é™©;å¯¹äºæ›´å¤šå¼‚æ„æ•°æ®è¿˜éœ€è¦ç»“åˆè€ƒè™‘;æœ¬æ–‡éƒ¨åˆ†å…¬å¼æœªç»æ¶ˆèå®éªŒäºŸéœ€æœ€ä¼˜è§£;æœ¬æ–‡çš„è®­ç»ƒé›†åŠéªŒè¯é›†ä»éœ€æ›´å¤§è§„æ¨¡çš„æ‰©å……;å…³é”®è¯è¿˜å¯ä»¥éšæ—¶é—´å¢åŠ è§„æ¨¡ç­‰ç­‰,åªå¯æƒœå¯èƒ½ä¸ä¼šå†ä¼˜åŒ–äº†ã€‚